{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eloimoliner/unconditional-diff-STFT/blob/main/colab/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unconditional synthesis of music using an STFT-based diffusion model"
      ],
      "metadata": {
        "id": "jbe_aWYkjWRH"
      },
      "id": "jbe_aWYkjWRH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this notebook, you can synthesize unconditional music using a diffusion model.\n",
        "\n",
        "I provide two pretrained models trained with different instruments:\n",
        "  - piano\n",
        "  - strings\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each of the cells\n",
        "* View the code: Double-click any of the cells\n",
        "* Hide the code: Double click the right side of the cell\n"
      ],
      "metadata": {
        "id": "8UON6ncSApA9"
      },
      "id": "8UON6ncSApA9"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Setup environment\n",
        "\n",
        "#@markdown Execute this cell to download the code and weights \n",
        "! git clone https://github.com/eloimoliner/unconditional-diff-STFT.git\n",
        "%cd unconditional-diff-STFT\n",
        "! wget https://github.com/eloimoliner/unconditional-diff-STFT/releases/download/weights_piano/weights_piano_uncond_synth.pt\n",
        "! mkdir experiments\n",
        "! mkdir experiments/piano\n",
        "! mv weights_piano_uncond_synth.pt experiments/piano/\n",
        "\n",
        "!pip install omegaconf\n",
        "! pip install dotmap\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wTvg36-LWXIK",
        "outputId": "cf48867d-7c78-4098-dc04-6b0fcbe62756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wTvg36-LWXIK",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'unconditional-diff-STFT'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 59 (delta 27), reused 49 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n",
            "/content/unconditional-diff-STFT/unconditional-diff-STFT/unconditional-diff-STFT\n",
            "--2022-05-19 14:08:04--  https://github.com/eloimoliner/unconditional-diff-STFT/releases/download/weights_piano/weights_piano_uncond_synth.pt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/493309012/32c2dd87-b60a-4c11-841c-0c0244a5c91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220519T140804Z&X-Amz-Expires=300&X-Amz-Signature=7e622e5564800441cfb9b7d7754b25f11767b4eac06eac504c8bba74f03037aa&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=493309012&response-content-disposition=attachment%3B%20filename%3Dweights_piano_uncond_synth.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-05-19 14:08:04--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/493309012/32c2dd87-b60a-4c11-841c-0c0244a5c91e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220519%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220519T140804Z&X-Amz-Expires=300&X-Amz-Signature=7e622e5564800441cfb9b7d7754b25f11767b4eac06eac504c8bba74f03037aa&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=493309012&response-content-disposition=attachment%3B%20filename%3Dweights_piano_uncond_synth.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1120584379 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘weights_piano_uncond_synth.pt’\n",
            "\n",
            "weights_piano_uncon 100%[===================>]   1.04G  66.2MB/s    in 15s     \n",
            "\n",
            "2022-05-19 14:08:19 (73.2 MB/s) - ‘weights_piano_uncond_synth.pt’ saved [1120584379/1120584379]\n",
            "\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (2.2.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf) (6.0)\n",
            "Requirement already satisfied: dotmap in /usr/local/lib/python3.7/dist-packages (1.3.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Imports and others\n",
        "\n",
        "#@markdown\n",
        "\n",
        "import soundfile as sf\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as npp\n",
        "import dataset_loader\n",
        "from omegaconf import OmegaConf\n",
        "from omegaconf.omegaconf import open_dict\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from getters import get_sde\n",
        "from unet_STFT import Unet2d\n",
        "import scipy.signal\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from dotmap import DotMap\n",
        "\n",
        "args = yaml.safe_load(Path('conf/conf.yaml').read_text())\n",
        "args = DotMap(args)\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dirname = os.getcwd()\n",
        "\n",
        "path_experiment= os.path.join(dirname, \"experiments/piano\")\n",
        "if not(os.path.exists(path_experiment)):\n",
        "    os.mkdir(path_experiment)\n",
        "\n",
        "args.model_dir=path_experiment\n",
        "\n",
        "\n",
        "model=Unet2d(args).to(device)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "sde = get_sde(args.sde_type, args.sde_kwargs)\n",
        "\n",
        "segment_size=args.audio_len\n",
        "\n",
        "class SDESampling_context:\n",
        "    \"\"\"\n",
        "    DDPM-like discretization of the SDE as in https://arxiv.org/abs/2107.00630\n",
        "    Using context, stereo...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, sde):\n",
        "        self.model = model\n",
        "        self.sde = sde\n",
        "\n",
        "    def create_schedules(self, nb_steps, stereo_split):\n",
        "        t_schedule = torch.arange(0, nb_steps + 1) / nb_steps\n",
        "        t_schedule = (self.sde.t_max - self.sde.t_min) * \\\n",
        "            t_schedule + self.sde.t_min\n",
        "        split= (self.sde.t_max - self.sde.t_min) * \\\n",
        "           stereo_split + self.sde.t_min\n",
        "        split=int(split*nb_steps)\n",
        "        sigma_schedule = self.sde.sigma(t_schedule)\n",
        "        m_schedule = self.sde.mean(t_schedule)\n",
        "\n",
        "        return sigma_schedule, m_schedule, split\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        contextL,\n",
        "        contextR,\n",
        "        mask,\n",
        "        nb_steps,\n",
        "        stereo=False,\n",
        "        stereo_split=0.05\n",
        "    ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            sigma, m ,stereo_split  = self.create_schedules(nb_steps, stereo_split)\n",
        "\n",
        "            #map audio to latent space \n",
        "\n",
        "            #start sampling from trunc\n",
        "            context=(contextL+contextR)/2\n",
        "            context_noisy = m[nb_steps-1] * context + sigma[nb_steps-1] * torch.randn_like(context)\n",
        "            audio=context_noisy\n",
        "\n",
        "            for n in range(nb_steps - 1, 0, -1):\n",
        "                # begins at t = 1 (n = nb_steps - 1)\n",
        "                # stops at t = 2/nb_steps (n=1)\n",
        "                #print(n)\n",
        "                #map context to latent space\n",
        "\n",
        "                audio = m[n-1] / m[n] * audio + (m[n] / m[n-1] * (sigma[n-1])**2 / sigma[n] - m[n-1] / m[n] * sigma[n]) * \\\n",
        "                    self.model(audio, sigma[n])\n",
        "\n",
        "                if n > 0:  # everytime\n",
        "                    noise = torch.randn_like(audio)\n",
        "                    audio += sigma[n-1]*(1 - (sigma[n-1]*m[n] /\n",
        "                                              (sigma[n]*m[n-1]))**2)**0.5 * noise\n",
        "                #map to latent space\n",
        "                context_noisy = m[n-1] * context + sigma[n-1] * torch.randn_like(context)\n",
        "\n",
        "                #combine context and no context\n",
        "                audio=(1-mask)*context_noisy+mask*audio\n",
        "                if stereo and n==stereo_split:\n",
        "                    audio_stereo=torch.clone(audio)\n",
        "                    context=contextL\n",
        "\n",
        "            # The noise level is now sigma(1/nb_steps) = sigma[0]\n",
        "            # Jump step\n",
        "            audio = (audio - sigma[0] * self.model(audio,\n",
        "                                                   sigma[0])) / m[0]\n",
        "        \n",
        "            audio=(1-mask)*context+mask*audio\n",
        "\n",
        "            if stereo:\n",
        "                audio_left=audio\n",
        "                audio=audio_stereo\n",
        "                context=contextR\n",
        "                for n in range(stereo_split - 1, 0, -1):\n",
        "            \n",
        "                    #print(n)\n",
        "                    #map context to latent space\n",
        "    \n",
        "                    audio = m[n-1] / m[n] * audio + (m[n] / m[n-1] * (sigma[n-1])**2 / sigma[n] - m[n-1] / m[n] * sigma[n]) * \\\n",
        "                        self.model(audio, sigma[n])\n",
        "    \n",
        "                    if n > 0:  # everytime\n",
        "                        noise = torch.randn_like(audio)\n",
        "                        audio += sigma[n-1]*(1 - (sigma[n-1]*m[n] /\n",
        "                                                  (sigma[n]*m[n-1]))**2)**0.5 * noise\n",
        "                    #map to latent space\n",
        "                    context_noisy = m[n-1] * context + sigma[n-1] * torch.randn_like(context)\n",
        "    \n",
        "                    #combine context and no context\n",
        "                    audio=(1-mask)*context_noisy+mask*audio\n",
        "    \n",
        "                # The noise level is now sigma(1/nb_steps) = sigma[0]\n",
        "                # Jump step\n",
        "                audio = (audio - sigma[0] * self.model(audio,\n",
        "                                                       sigma[0])) / m[0]\n",
        "            \n",
        "                audio=(1-mask)*context+mask*audio\n",
        "                audio_right=audio\n",
        "                return audio_left, audio_right\n",
        "            else:    \n",
        "                return audio\n"
      ],
      "metadata": {
        "id": "oRAKBu5vXRG2"
      },
      "id": "oRAKBu5vXRG2",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Generate music\n",
        "\n",
        "\n",
        "#@markdown This may take a while, be patient\n",
        "\n",
        "\n",
        "checkpoint=\"weights_piano_uncond_synth.pt\"\n",
        "model_dir = os.path.join(path_experiment, checkpoint) #hardcoded for now\n",
        "state_dict= torch.load(model_dir, map_location=device)\n",
        "\n",
        "if hasattr(model, 'module') and isinstance(model.module, nn.Module):\n",
        "    model.module.load_state_dict(state_dict['model'])\n",
        "else:\n",
        "    model.load_state_dict(state_dict['model'])\n",
        "\n",
        "sampler=SDESampling_context(model, sde)\n",
        "\n",
        "overlapsize=int(args.audio_len/4)\n",
        "numchunks=10\n",
        "T=20\n",
        "args.inference.stereo=True\n",
        "pointer=0\n",
        "\n",
        "for i in tqdm(range(numchunks)):\n",
        "    if i==0:\n",
        "        if args.inference.stereo:\n",
        "            contextL=torch.zeros((1,segment_size)).to(device)\n",
        "            contextR=torch.zeros((1,segment_size)).to(device)\n",
        "            mask=torch.ones((1,segment_size)).to(device)\n",
        "        else:\n",
        "            context=torch.zeros((1,segment_size)).to(device)\n",
        "            mask=torch.ones((1,segment_size)).to(device)\n",
        "    else:\n",
        "        if args.inference.stereo:\n",
        "            mask=torch.cat((torch.zeros((1,overlapsize)),torch.ones((1,segment_size-overlapsize))),dim=1).to(device)\n",
        "            contextL=torch.cat((predL[:,segment_size-overlapsize::],torch.zeros((1,segment_size-overlapsize)).to(device)),dim=1).to(device)\n",
        "            contextR=torch.cat((predR[:,segment_size-overlapsize::],torch.zeros((1,segment_size-overlapsize)).to(device)),dim=1).to(device)\n",
        "\n",
        "        else:\n",
        "            mask=torch.cat((torch.zeros((1,overlapsize)),torch.ones((1,segment_size-overlapsize))),dim=1).to(device)\n",
        "            context=torch.cat((pred[:,segment_size-overlapsize::],torch.zeros((1,segment_size-overlapsize)).to(device)),dim=1).to(device)\n",
        "\n",
        "    if args.inference.stereo:\n",
        "        predL, predR=sampler.predict(contextL, contextR, mask, T, stereo=True, stereo_split=0.05)\n",
        "        pred_2=torch.stack((predL.squeeze(0), predR.squeeze(0)), dim=1)\n",
        "    else:\n",
        "        pred=sampler.predict(context, context, mask, T, stereo=False)\n",
        "        pred_2=pred.squeeze(0)\n",
        "\n",
        "    if i==0:\n",
        "        bwe_data=pred_2\n",
        "    else:\n",
        "        bwe_data=torch.cat((bwe_data,pred_2[overlapsize::]),dim=0)\n",
        "\n",
        "    pointer=pointer+segment_size-overlapsize\n",
        "\n",
        "bwe_data=bwe_data.cpu().numpy()\n",
        "wav_output_name=\"unconditional.wav\"\n",
        "sf.write(wav_output_name, bwe_data, args.sample_rate)\n"
      ],
      "metadata": {
        "id": "v1lhCN8uj4Ug",
        "outputId": "4cff564c-d42e-4349-9dc4-84237912ecd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v1lhCN8uj4Ug",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:48<00:00, 22.86s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPFVe3fFrHLV"
      },
      "id": "QPFVe3fFrHLV",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Download\n",
        "\n",
        "#@markdown Execute this cell to download the generated music\n",
        "from google.colab import files\n",
        "files.download(wav_output_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3tEshWBezYvf",
        "outputId": "a174e7c7-9ae0-4fb7-cab2-a058c5457f86"
      },
      "id": "3tEshWBezYvf",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_313a3823-84e6-4dd1-b900-d19a14fcb58c\", \"unconditional.wav\", 2031660)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v_FuSJ4J-WO-"
      },
      "id": "v_FuSJ4J-WO-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}